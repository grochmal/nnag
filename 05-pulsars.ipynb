{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Neural Networks \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulsars\n",
    "\n",
    "Until now we have been quite theoretical.\n",
    "We will now introduce a dataset which we will train networks on.\n",
    "The pulsar dataset contains data on 17898 stars,\n",
    "from which 1639 are pulsars (high momentum neutron stars - or white dwarfs).\n",
    "\n",
    "Since the original dataset is imbalanced, we have tuned the data to contain\n",
    "the same number of pulsars and non-pulsars.  The file `./pulsars_raw.csv`\n",
    "contains the original dataset.  Whilst the file `./pulsars_tuned.csv` contains\n",
    "the tuned dataset with balanced classes.  We will always us the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3278, 8) (3278,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./pulsars_tuned.csv')\n",
    "X, y = df.values[:, :-1], df['label'].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_mean</th>\n",
       "      <th>ip_std</th>\n",
       "      <th>ip_kurtosis</th>\n",
       "      <th>ip_skewness</th>\n",
       "      <th>dmsnr_mean</th>\n",
       "      <th>dmsnr_std</th>\n",
       "      <th>dmsnr_kurtosis</th>\n",
       "      <th>dmsnr_skewness</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.50781</td>\n",
       "      <td>58.88243</td>\n",
       "      <td>0.46532</td>\n",
       "      <td>-0.51509</td>\n",
       "      <td>1.67726</td>\n",
       "      <td>14.86015</td>\n",
       "      <td>10.57649</td>\n",
       "      <td>127.39358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142.07812</td>\n",
       "      <td>45.28807</td>\n",
       "      <td>-0.32033</td>\n",
       "      <td>0.28395</td>\n",
       "      <td>5.37625</td>\n",
       "      <td>29.00990</td>\n",
       "      <td>6.07627</td>\n",
       "      <td>37.83139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138.17969</td>\n",
       "      <td>51.52448</td>\n",
       "      <td>-0.03185</td>\n",
       "      <td>0.04680</td>\n",
       "      <td>6.33027</td>\n",
       "      <td>31.57635</td>\n",
       "      <td>5.15594</td>\n",
       "      <td>26.14331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114.36719</td>\n",
       "      <td>51.94572</td>\n",
       "      <td>-0.09450</td>\n",
       "      <td>-0.28798</td>\n",
       "      <td>2.73829</td>\n",
       "      <td>17.19189</td>\n",
       "      <td>9.05061</td>\n",
       "      <td>96.61190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.36719</td>\n",
       "      <td>41.57220</td>\n",
       "      <td>1.54720</td>\n",
       "      <td>4.15411</td>\n",
       "      <td>27.55518</td>\n",
       "      <td>61.71902</td>\n",
       "      <td>2.20881</td>\n",
       "      <td>3.66268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ip_mean    ip_std  ip_kurtosis  ip_skewness  dmsnr_mean  dmsnr_std  \\\n",
       "0  102.50781  58.88243      0.46532     -0.51509     1.67726   14.86015   \n",
       "1  142.07812  45.28807     -0.32033      0.28395     5.37625   29.00990   \n",
       "2  138.17969  51.52448     -0.03185      0.04680     6.33027   31.57635   \n",
       "3  114.36719  51.94572     -0.09450     -0.28798     2.73829   17.19189   \n",
       "4   99.36719  41.57220      1.54720      4.15411    27.55518   61.71902   \n",
       "\n",
       "   dmsnr_kurtosis  dmsnr_skewness  label  \n",
       "0        10.57649       127.39358      0  \n",
       "1         6.07627        37.83139      0  \n",
       "2         5.15594        26.14331      0  \n",
       "3         9.05061        96.61190      0  \n",
       "4         2.20881         3.66268      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the first four statistical moments of two indicators\n",
    "\n",
    "IP: Integrated Profile.\n",
    "The profile (amount of light) of a pulse is often very different between pulses.\n",
    "Integrated simply means we baseline the pulse so we have an intensity of zero\n",
    "anywhere else but the pulses.\n",
    "It is expected that pulsars have high variance (and standard deviation)\n",
    "between pulses (between integrated profiles of pulses).\n",
    "\n",
    "DM-SNR: Dispersion Measure, Signal to Noise Ratio.\n",
    "Where dispersion means a delay in the lower frequencies, due to interaction with free electrons.\n",
    "In other words, higher frequencies arrive faster then lower frequencies,\n",
    "the dispersion is in the time of different frequency arrival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN with `pytorch`\n",
    "\n",
    "We will train an ANN on this dataset.\n",
    "For this purpose we will use `pytorch`,\n",
    "the objective of this exercise will be to prove that `pytorch` performs the exact\n",
    "matrix multiplications we performed in the concepts section.\n",
    "\n",
    "For a start we need to modify `y` a little.\n",
    "We have two outputs: pulsar and/or non-pulsar.\n",
    "In the data pulsar is indicated with a $1$ and non-pulsar with a $0$.\n",
    "Yet, ANNs work better if we have each output neuron output just a single class.\n",
    "In this case we want to represent pulsars as $[0, 1]$ and non-pulsars as $[1, 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3278, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.c_[y == 0, y == 1].astype(np.float)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code looks very similar to `pytorch` tutorials on ANNs.\n",
    "This is intentional, our objective here is not to learn `pytorch`\n",
    "but instead to understand what it does below the hood.\n",
    "That said, below we implement *Stochastic Gradient Descent* (SGD) by hand.\n",
    "There are other ways to train a network but most are variants of\n",
    "*Gradient Descent*, we will use (SGD) in all our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1832, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1631, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1124, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1028, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0937, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0999, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0628, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0808, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0976, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0620, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0467, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0761, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0697, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0716, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 25)\n",
    "        self.fc2 = nn.Linear(25, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "batch = 100\n",
    "for i in range(1000):\n",
    "    idx = np.random.randint(0, len(y), batch)\n",
    "    X_sample, y_sample = torch.Tensor(X[idx]), torch.Tensor(y[idx])\n",
    "\n",
    "    net.zero_grad()\n",
    "    y_hat = net(X_sample)\n",
    "    loss = criterion(y_hat, y_sample)\n",
    "    loss.backward()\n",
    "    if 0 == (i+1)%50:\n",
    "        print(loss)\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of checking whether an ANN has converged:\n",
    "having a tolerance of error, tolerance of gradient,\n",
    "or continually reducing the learning rate; yet we will ignore their\n",
    "complexities and verify network convergence visually.\n",
    "Namely: if the *Mean Square Error* (MSE) is reducing the network is learning something,\n",
    "if the MSE is reducing continuously then we accept that we have a trained network.\n",
    "\n",
    "The simplification will be good enough for our purposes.\n",
    "One thing we may do to prove to ourselves that the network did learn\n",
    "is to check how many sample it did identify correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87614399023795\n",
      "0.9438682123245882\n",
      "0.910006101281269\n"
     ]
    }
   ],
   "source": [
    "out = net(torch.Tensor(df.values[:, :-1]))\n",
    "y_hat = out.argmax(dim=1).numpy()\n",
    "y_true = df['label'].values\n",
    "print(sum(y_hat[y_true == 1] == y_true[y_true == 1])/sum(y_true == 1))\n",
    "print(sum(y_hat[y_true == 0] == y_true[y_true == 0])/sum(y_true == 0))\n",
    "print(sum(y_hat == y_true)/len(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ignoring for the time being overfitting of the network.\n",
    "It is likely that our ANN is overfitting,\n",
    "and this can be solved by providing test and validation sets.\n",
    "Yet, to keep it simple we will ignore this need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equations\n",
    "\n",
    "All the network above is doing is matrix multiplication.\n",
    "We keep repeating that but this is really important!\n",
    "We used a network with three layers: one with 25 neurons,\n",
    "one with 10 neurons and one with 2 neurons (pulsar or not pulsar).\n",
    "We also have 8 features from our dataset.\n",
    "\n",
    "A matrix is often written with the number of its rows and columns.\n",
    "For example matrix $A$ with $p$ rows and $n$ columns can be written:\n",
    "\n",
    "$$\n",
    "A_{n x p}\n",
    "$$\n",
    "\n",
    "This means that we have the following matrices in the ANN:\n",
    "\n",
    "$$\n",
    "W_{8 \\times 20}, W_{B\\: 20 \\times 1},\n",
    "W'_{20 \\times 10}, W'_{B\\: 10 \\times 1},\n",
    "W'_{10 \\times 2}, W'_{B\\: 2 \\times 1}\n",
    "$$\n",
    "\n",
    "We can see these matrices in the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 8), (25,), (10, 25), (10,), (2, 10), (2,)]\n"
     ]
    }
   ],
   "source": [
    "ws = []\n",
    "for f in net.parameters():\n",
    "    ws.append(f.data.numpy())\n",
    "print(list(map(lambda x: x.shape, ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we did use above is the activation function (*tanh()*).\n",
    "If we multiply an input vector through the matrices\n",
    "and apply the activation function after each multiplication\n",
    "we should get the same output as `pytorch` gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8634, 0.1527], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(net(torch.Tensor(X[0, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our multiplication of:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\tanh(W'_{10 \\times 2} \\cdot\n",
    "  \\tanh(W'_{20 \\times 10} \\cdot\n",
    "    \\tanh(W_{8 \\times 20} \\cdot \\vec{x} + W_{B\\: 20 \\times 1})\n",
    "    + W'_{B\\: 10 \\times 1})\n",
    "  + W'_{B\\: 2 \\times 1}\n",
    "$$\n",
    "\n",
    "Gives the exact same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86337939 0.15266306]\n"
     ]
    }
   ],
   "source": [
    "W = ws[::2]\n",
    "Wb = ws[1::2]\n",
    "vector = X[0, :]\n",
    "for w, b in zip(W, Wb):\n",
    "    vector = np.tanh(w @ vector + b)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a little more to it though.\n",
    "\n",
    "We normally think of data as rows meaning samples and columns meaning features.\n",
    "That is all good but in our equation above and in the code we used the\n",
    "vector as a column vector.\n",
    "In other words, we had one column and eight rows.\n",
    "`numpy` was kind enough to figure it out and perform the transpose for us.\n",
    "\n",
    "Now, we normally do not feed a single sample into a neural network,\n",
    "instead we feed batches (above we used a batch of 100 sample at a time).\n",
    "The `pytorch` network performs all the transformations needed internally,\n",
    "we can feed it several rows and it predicts all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8634,  0.1527],\n",
      "        [ 0.8261, -0.1086],\n",
      "        [ 0.8253, -0.1033]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(net(torch.Tensor(X[:3, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we are going to feed several samples into our code above,\n",
    "we will need to perform the transposes ourselves.\n",
    "\n",
    "Let's remind ourselves that two matrices can only multiply\n",
    "if the number of columns of the left matrix is the same as the number\n",
    "of rows of the right matrix.\n",
    "Moreover the resulting matrix has the number of rows equal to the number\n",
    "of rows of the left matrix and the number of columns equal to the number\n",
    "of columns of the right matrix.\n",
    "\n",
    "$$\n",
    "A_{n \\times p} \\cdot B_{p \\times q} = C_{n \\times q}\n",
    "$$\n",
    "\n",
    "Let's say we feed 3 samples and note that:\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{2 \\times 3} = tanh(W''_{2 \\times 10} \\times\n",
    "    tanh(W'_{10 \\times 25} \\times\n",
    "        tanh(W_{25 \\times 8} \\times X_{8 \\times 3} + W_{B\\: 25 \\times 1})\n",
    "    + W'_{B\\: 10 \\times 1})\n",
    "+ W''_{B\\: 2 \\times 1})\n",
    "$$\n",
    "\n",
    "But in our dataset we have $X_{3278 \\times 8}$ and we need it to have\n",
    "$8$ rows, not columns, to fit in the equation above.\n",
    "The same goes for $\\hat{Y}$.  We want the answers to be a row per input row,\n",
    "but we are getting a column per input.  We need to transpose both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86337939  0.15266306]\n",
      " [ 0.82607889 -0.10857165]\n",
      " [ 0.82526583 -0.10333694]]\n"
     ]
    }
   ],
   "source": [
    "W = ws[::2]\n",
    "Wb = ws[1::2]\n",
    "vector = X[:3, :].T\n",
    "for w, b in zip(W, Wb):\n",
    "    vector = np.tanh(w @ vector + b[:, np.newaxis])\n",
    "print(vector.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we can argue that we understand how `pytorch` executes its ANNs.\n",
    "We still do not know how it calculates its gradients but we will be on that next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
