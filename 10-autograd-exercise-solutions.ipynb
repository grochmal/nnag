{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Neural Networks \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises - Autograd\n",
    "\n",
    "Let's do some differentiation!\n",
    "\n",
    "For a start install `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "conda install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "pip isntall autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function inside `autograd` is `grad`.\n",
    "It differentiates a function at a point it has been evaluated\n",
    "with relation to it first argument\n",
    "(or another argument if you pass `argnum=`).\n",
    "Also `autograd` wraps `numpy` so that `numpy` arrays\n",
    "can be differentiated with relation to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an example, we will take the function\n",
    "\n",
    "$$\n",
    "f(x) = (x - 3)^2\n",
    "$$\n",
    "\n",
    "And we know that\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 2(x - 3)\n",
    "$$\n",
    "\n",
    "Now we will use `grad` to do this for us.\n",
    "But `grad` evaluates the derivative at a point, so we will use $x = 7$.\n",
    "\n",
    "Note: **`autograd` only works on floating point numbers**,\n",
    "don't be caught by literally writing integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return (x - 3)**2\n",
    "\n",
    "fun_grad = grad(fun)\n",
    "x = 7.\n",
    "print(fun(x))\n",
    "print(fun_grad(7.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which seems about correct, $(7 - 3)^2 = 16$ and $2(7 - 3) = 8$.\n",
    "Let's try this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Evaluate the function $f(x) = (x - 10)^2$ and its derivative (using `autograd`) at\n",
    "\n",
    "A) $x = 3$\n",
    "\n",
    "B) $x = 10$\n",
    "\n",
    "C) $x = 12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.0\n",
      "0.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x - 10)**2\n",
    "\n",
    "f_grad = grad(f)\n",
    "print(f_grad(3.))\n",
    "print(f_grad(10.))\n",
    "print(f_grad(12.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Evaluate, on paper, the derivative of $f(x) = (x - 10)^2$\n",
    "\n",
    "Note that in this case we can mostly ignore the existence of the $-10$\n",
    "and use the basic rule of derivative powers:\n",
    "\n",
    "$$\n",
    "\\frac{dx^n}{dx} = n \\cdot x^{(n - 1)}\n",
    "$$\n",
    "\n",
    "Do it by analogy with the derivative of $(x - 3)^2$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d(x - 10)^2}{dx} = 2(x - 10)\n",
    "$$\n",
    "\n",
    "And we can see that $2(3 - 10) = -14$, $2(10 - 10) = 0$ and $2(12 - 10) = 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use `autograd` to evaluate the (slightly more complex) derivatives of the following functions\n",
    "\n",
    "A) $f(x) = xe^x$ at $x = 7$\n",
    "\n",
    "B) $f(x) = \\frac{x + 2^x}{e^x}$ at $x = 3.5$\n",
    "\n",
    "C) $f(x) = \\ln(2 + x) + cos^2(x)$ at $x = -0.5$\n",
    "\n",
    "Note: $e$ is the Euler's number and exists in `numpy` as `np.e`,\n",
    "$\\ln$ is the natural logarithm - with $e$ as its base - in `numpy`\n",
    "`np.log` performs the natural logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7676.432108999208 8773.065267427666\n",
      "0.44733523545030124 -0.18032800393862408\n",
      "1.0444776022734237 0.2978942899546265\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x * np.e**x\n",
    "\n",
    "def f2(x):\n",
    "    return (x + 2**x)/(np.e**x)\n",
    "\n",
    "def f3(x):\n",
    "    return np.log(2 + x) + np.cos(np.cos(x))\n",
    "\n",
    "f1_grad = grad(f1)\n",
    "f2_grad = grad(f2)\n",
    "f3_grad = grad(f3)\n",
    "print(f1(7.), f1_grad(7.))\n",
    "print(f2(3.5), f2_grad(3.5))\n",
    "print(f3(-0.5), f3_grad(-0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`autograd` differentiates only one of the arguments (by default the first).\n",
    "Yet, that argument may be a list or other sequence,\n",
    "that way we are able to differentiate multidimensional functions.\n",
    "For example we ma have\n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 + 3y\n",
    "$$\n",
    "\n",
    "And we want to differentiate with relation to (wrt) both $x$ and $y$.\n",
    "We can do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "[array(6.), array(3.)]\n"
     ]
    }
   ],
   "source": [
    "def f(args):\n",
    "    x, y = args\n",
    "    return x**2 + 3*y\n",
    "\n",
    "f_grad = grad(f)\n",
    "args = [3., 2.]\n",
    "print(f(args))\n",
    "print(f_grad(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got some extra `numpy` artifacts in there\n",
    "but in general `autograd` did compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 2x\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = 3\n",
    "$$\n",
    "\n",
    "And got it right, $2*3 = 6$ and $3 = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate all three partial derivatives for $f(x, y, z) = x^2y3^z$ at $x = 3$, $y = -1$ and $z = 2$\n",
    "\n",
    "i.e. evaluate $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$\n",
    "and $\\frac{\\partial f}{\\partial z}$ using `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-81.0\n",
      "[array(-54.), array(81.), array(-88.98759538)]\n"
     ]
    }
   ],
   "source": [
    "def f(args):\n",
    "    x, y, z = args\n",
    "    return x**2*y*3**z\n",
    "\n",
    "f_grad = grad(f)\n",
    "print(f([3., -1., 2.]))\n",
    "print(f_grad([3., -1., 2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. (optional) Verify the results from the previous exercise on paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Evaluate the derivative of matrix multiplication\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "W = \\left[\n",
    "\\begin{matrix}\n",
    "0.3 & -0.2 & 0.1 \\\\\n",
    "0.1 &  0.2 & 0.1 \\\\\n",
    "\\end{matrix}\n",
    "\\right],\n",
    "V = \\left[\n",
    "\\begin{matrix}\n",
    "0.1 & -0.3 \\\\\n",
    "0.1 &  0.5 \\\\\n",
    "0.2 &  0.7 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Use `autograd` to evaluate  $\\frac{\\partial f}{\\partial W}$\n",
    "and $\\frac{\\partial f}{\\partial V}$ for the functions\n",
    "\n",
    "A) $f(W, V) = mean(W \\cdot V)$\n",
    "\n",
    "B) $f(W, V) = mean(W \\cdot V)$\n",
    "\n",
    "Note: are the results the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([\n",
    "    [.3, -.2, .1],\n",
    "    [.1,  .2, .1],\n",
    "])\n",
    "V = np.array([\n",
    "    [.1, -.3],\n",
    "    [.1,  .5],\n",
    "    [.2,  .7],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025\n",
      "0.048888888888888885\n",
      "[array([[-0.05 ,  0.15 ,  0.225],\n",
      "       [-0.05 ,  0.15 ,  0.225]]), array([[0.1 , 0.1 ],\n",
      "       [0.  , 0.  ],\n",
      "       [0.05, 0.05]])]\n",
      "[array([[0.04444444, 0.04444444, 0.04444444],\n",
      "       [0.1       , 0.1       , 0.1       ]]), array([[0.02222222, 0.04444444],\n",
      "       [0.02222222, 0.04444444],\n",
      "       [0.02222222, 0.04444444]])]\n"
     ]
    }
   ],
   "source": [
    "def f1(args):\n",
    "    W, V = args\n",
    "    return np.mean(W @ V)\n",
    "\n",
    "def f2(args):\n",
    "    W, V = args\n",
    "    return np.mean(V @ W)\n",
    "\n",
    "f1_grad = grad(f1)\n",
    "f2_grad = grad(f2)\n",
    "print(f1([W, V]))\n",
    "print(f2([W, V]))\n",
    "print(f1_grad([W, V]))\n",
    "print(f2_grad([W, V]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
