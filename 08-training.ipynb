{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Neural Networks \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an ANN\n",
    "\n",
    "We already said that we were using *Stochastic Gradient Descent* (SDG)\n",
    "to train the network.  But what that SDG actually is.\n",
    "\n",
    "The *Stochastic* bit just means that we use a random sample as a batch\n",
    "at every step in the training.  We have an example of this in our `pytorch` ANN.\n",
    "But the *Gradient Descent* is more mathematical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN the function\n",
    "\n",
    "Whatever an ANN looks like we can say that it takes a multidimensional input\n",
    "and spits a multidimensional output.\n",
    "We generated completely random matrices and they worked as an ANN.\n",
    "The random matrices produced completely random outputs\n",
    "but produced outputs in the correct number of dimensions.\n",
    "\n",
    "In other words the only difference between an untrained network and\n",
    "a trained network are the values of weights.\n",
    "Well, we kind of knew that already but now we can see it in mathematical terms.\n",
    "\n",
    "Since we just said that both a trained and untrained ANN is something\n",
    "which given a multidimensional input gives a multidimensional output,\n",
    "we can argue that an ANN can be understood as a function.\n",
    "A function parametrized by all the weights inside all the matrices.\n",
    "Specifically we call the action of out ANN $N$ and say:\n",
    "\n",
    "$$\n",
    "N_{w_1, w_2, \\dots, w_n}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "For the case of our ANN dealing with the pulsars dataset we have:\n",
    "\n",
    "$$\n",
    "N_{w_1, w_2, \\dots, w_n}: \\mathbb{R}^8 \\rightarrow \\mathbb{R}^2\n",
    "$$\n",
    "\n",
    "We also say that our ANN is a model, i.e. an estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\vec{y}} = N_{w_1, w_2, \\dots, w_n}(\\vec{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we imagine that out there exists a perfect model of our data.\n",
    "We do not know the perfect model but we know the values it would output.\n",
    "In the case of pulsars we know that for a certain input we have\n",
    "$1$ for pulsar and $0$ for non-pulsar.\n",
    "Or more exactly $[0, 1]$ for pulsar and $[1, 0]$ for non-pulsar\n",
    "since the output it 2-dimensional.  We call this output $\\vec{y}$ (the label).\n",
    "\n",
    "The difference between the correct label and our estimated label is the error\n",
    "out ANN is performing.\n",
    "\n",
    "$$\n",
    "E = \\vec{y} - \\hat{\\vec{y}}\n",
    "$$\n",
    "\n",
    "There's a problem here though.\n",
    "Since the error can be positive or negative it is difficult to compare two errors.\n",
    "Therefore we use the squared error.\n",
    "\n",
    "$$\n",
    "SE = (\\vec{y} - \\hat{\\vec{y}})^2\n",
    "$$\n",
    "\n",
    "Now we can define the function $F$ as follows:\n",
    "\n",
    "$$\n",
    "F_{w_1, w_2, \\dots, w_n} = (\\vec{y} -  N_{w_1, w_2, \\dots, w_n}(\\vec{x}))^2\n",
    "$$\n",
    "\n",
    "And this has a nice property that,\n",
    "if $F$ decreases out ANN is getting better, if $F$ increases our ANN is getting worse!\n",
    "So all we need to do is to change the values of the weights until\n",
    "we get a minimum value of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending $F$\n",
    "\n",
    "As we saw when we wrote the SGD ourselves,\n",
    "we never train the ANN with a single sample.\n",
    "We train it with a small batch of samples at a time.\n",
    "\n",
    "Therefore we are not really using $\\vec{y}$ as the comparison.\n",
    "Instead we are using several $\\vec{y}$ together, we will call it $Y$,\n",
    "a matrix with each column containing a $\\vec{y}$.\n",
    "With that in mind we are also not using $\\vec{x}$ but several samples at a time.\n",
    "We will write $X$, a matrix with $\\vec{x}$ as columns.\n",
    "\n",
    "Finally we write $F$ as:\n",
    "\n",
    "$$\n",
    "F_{w_1, w_2, \\dots, w_n} = (Y -  N_{w_1, w_2, \\dots, w_n}(X))^2\n",
    "$$\n",
    "\n",
    "There's a problem with this though, since the squared error was\n",
    "$(\\vec{y} - \\hat{\\vec{y}})^2$ the output of $F$ is not an error anymore,\n",
    "it is several errors.\n",
    "Nothing too difficult to solve, we can just get the mean of all those values,\n",
    "resulting in yet another approach to $F$\n",
    "\n",
    "$$\n",
    "MSE = F_{w_1, w_2, \\dots, w_n} = \\text{mean}(Y -  N_{w_1, w_2, \\dots, w_n}(X))^2\n",
    "$$\n",
    "\n",
    "This is often called the *Mean Squared Error* measure.\n",
    "Other error functions (e.g. cross-entropy) exist but for simplicity\n",
    "we will stick with MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Now that we have a function $F$ that mirrors the behavior of our ANN\n",
    "we could perturb the weights until we find a minimum.\n",
    "Yet, there is a better way.\n",
    "We can use the following fact.\n",
    "\n",
    "> The *sign* of the partial derivative of a function wrt. one of its parameters\n",
    "> points in the direction the function is increasing in that dimension.\n",
    "\n",
    "So, for every single weight we have a possible dimension in which to tune our function.\n",
    "And for every one of those dimensions (say, dimension $w_1$) we can compute\n",
    "\n",
    "$$\n",
    "g_1 = \\frac{\\partial MSE}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "And we know that the function increases in the direction of $g_1$.\n",
    "But we want to find a minimum, so we also know that the function\n",
    "decreases in the direction of $-g_1$.\n",
    "\n",
    "This technique is called *Gradient Descent* because it is described through\n",
    "the calculation of the gradient.  The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla MSE_{w_1, w_2, \\dots, w_n} =\n",
    "\\left[\n",
    "\\frac{\\partial MSE}{w_1},\n",
    "\\frac{\\partial MSE}{w_2},\n",
    "\\dots,\n",
    "\\frac{\\partial MSE}{w_n},\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the gradient gives us the partial derivatives against every single weight.\n",
    "The gradient therefore gives us the direction in which to go in order to make the ANN\n",
    "perform better, it does not give us how far we need to go though.\n",
    "Since the gradient may be quite large sometimes we then multiply it by a small constant\n",
    "to make sure we do not wander too far.  This small constant is called the *learning rate*.\n",
    "\n",
    "If we'd be able to flatten out all weights into an array we could write\n",
    "\n",
    "$$\n",
    "[w_1, w_2, \\dots, w_n] = [w_1, w_2, \\dots, w_n] - \\alpha \\cdot \\nabla MSE_{w_1, w_2, \\dots, w_n}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "Note: the actual implementation keeps the gradients together with the weights,\n",
    "in the same matrices.  Next we will look at `autograd` which is an implementation\n",
    "that allows one to calculate the gradients and extends `numpy` arrays to\n",
    "keep the computed gradients together with the weight matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
